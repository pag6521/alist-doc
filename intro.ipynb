{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "intro.ipynb",
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LL227x1Fox0u"
      },
      "source": [
        "# World Models Library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnsUwXPopVUd"
      },
      "source": [
        "World Models Library is a pure python tool facilitating planning from pixels. The major components are the following:\n",
        "*   **World Model:** A *world model* is an stateless model of the environment that is used for planning. Any world model should implement the following three methods:\n",
        "  *   `reset_fn(**kwargs) -> state`. This function is responsible for resetting the state of the world model and is called at the beginning of every episode. The *state* can have any structure.\n",
        "  *   `observe(last_frames, last_actions, last_rewards, state) -> state`. This function is responsible for updating the state of the world model given the latest observed changes in the environment.\n",
        "  *   `predict_fn(future_actions, state) -> predictions`. This function predicts the future and will be used by planners to evaluate action proposals. The *predictions* should be compatible with the `objective_fn` that is passed to the planner. This function should not change the input *state*.\n",
        "\n",
        "  Any state that is required for the operation of the model (e.g. recurrent layer state) should be a part of `state` object that is passed around and returned from `observe_fn`.\n",
        "*   **Planner:** A *planner* decides what actions to take next with the help of a *world model*. The planner is responsible to keep track of the world model's *state* and call `reset_fn`, `observe_fn` and `predict_fn` to interact with the *world model*.\n",
        "*   **Task:** A *task* is a thin wrapper around `gym.Env` that adds a name to the underlying environment and provides convinient factory methods to instantiate environments.\n",
        "\n",
        "![World Models Library Components](https://i.imgur.com/JmuCcRI.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmWvr-VW_yak"
      },
      "source": [
        "## Colab Setup\n",
        "\n",
        "This colab requires a **GPU runtime** to work since it uses EGL rendering backend."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8qRbvlP6eML",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d08b67c9-ac8b-4acf-8d4a-3f061bc59b4b"
      },
      "source": [
        "#@title Install MuJoco (Edit to add your license key)\n",
        "mjkey = \"\"\"\n",
        "\n",
        "MuJoCo License Here\n",
        "\n",
        "\"\"\".strip()\n",
        "\n",
        "mujoco_dir = \"$HOME/.mujoco\"\n",
        "\n",
        "# Install OpenGL deps\n",
        "!apt-get update && apt-get install -y --no-install-recommends \\\n",
        "  libgl1-mesa-glx libosmesa6 libglew2.0\n",
        "\n",
        "# Fetch MuJoCo binaries from Roboti\n",
        "!wget -q https://www.roboti.us/download/mujoco200_linux.zip -O mujoco.zip\n",
        "!unzip -o -q mujoco.zip -d \"$mujoco_dir\"\n",
        "\n",
        "# Copy over MuJoCo license\n",
        "!echo \"$mjkey\" > \"$mujoco_dir/mjkey.txt\"\n",
        "\n",
        "\n",
        "# Configure dm_control to use the OSMesa rendering backend\n",
        "%env MUJOCO_GL=egl\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.82)] [Connecting to security.ub\r                                                                               \rHit:2 https://cli.github.com/packages stable InRelease\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.82)] [\r                                                                               \rHit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "E: Unable to locate package libglew2.0\n",
            "E: Couldn't find any package by glob 'libglew2.0'\n",
            "E: Couldn't find any package by regex 'libglew2.0'\n",
            "env: MUJOCO_GL=egl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5340xDeR0_-F",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "outputId": "6a8e15f0-f095-49bb-db24-072af53fd9cb"
      },
      "source": [
        "#@title Imports\n",
        "!pip install git+https://github.com/google-research/world_models.git\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "from world_models.simulate import simulate\n",
        "from world_models.agents import planet\n",
        "from world_models.planners import planners\n",
        "from world_models.objectives import objectives\n",
        "from world_models.simulate import simulate\n",
        "from world_models.utils import npz\n",
        "from world_models.loops import train_eval\n",
        "from world_models.tasks import tasks\n",
        "\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/google-research/world_models.git\n",
            "  Cloning https://github.com/google-research/world_models.git to /tmp/pip-req-build-esezxoxh\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/google-research/world_models.git /tmp/pip-req-build-esezxoxh\n",
            "  Resolved https://github.com/google-research/world_models.git to commit 9c09655e3d10ce5fa798a46feea19069c39acf07\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from world_models==1.0.0) (1.4.0)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.12/dist-packages (from world_models==1.0.0) (0.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from world_models==1.0.0) (2.0.2)\n",
            "INFO: pip is looking at multiple versions of world-models to determine which version is compatible with other requirements. This could take a while.\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==1.15 (from world-models) (from versions: 2.16.0rc0, 2.16.1, 2.16.2, 2.17.0rc0, 2.17.0rc1, 2.17.0, 2.17.1, 2.18.0rc0, 2.18.0rc1, 2.18.0rc2, 2.18.0, 2.18.1, 2.19.0rc0, 2.19.0, 2.19.1, 2.20.0rc0, 2.20.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==1.15\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'world_models'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1347834118.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mworld_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msimulate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mworld_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplanet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mworld_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplanners\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplanners\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'world_models'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKqMAbryQGyt"
      },
      "source": [
        "## Task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evdKt-cxQL45"
      },
      "source": [
        "As a first step, we should choose a task to solve. There are several task suites already defined in the `tasks/tasks.py` file. We will use DeepMind Control's Cheetah task as example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "236d2dUpRtsm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "b8c68a3b-84a4-4492-94ae-597c7533c756"
      },
      "source": [
        "task = tasks.DeepMindControl(domain_name='cheetah',\n",
        "                             task_name='run',\n",
        "                             action_repeat=4)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tasks' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4021948495.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m task = tasks.DeepMindControl(domain_name='cheetah',\n\u001b[0m\u001b[1;32m      2\u001b[0m                              \u001b[0mtask_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'run'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                              action_repeat=4)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tasks' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SJQA9W8MB0i"
      },
      "source": [
        "## World Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KdmiBUSMGZ7"
      },
      "source": [
        "There are already a few options available defined in the `agents` folder including *PlaNet*, *SV2P*, etc. For this colab we will instantiate a *PlaNet* agent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "if9Xmh_jJEhx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "fca93046-30e4-4bd3-d8b3-fb885a847cf3"
      },
      "source": [
        "model = planet.RecurrentStateSpaceModel(task=task)\n",
        "model_dir = '/tmp/experiment/model'\n",
        "dist_strategy = tf.distribute.MirroredStrategy()\n",
        "\n",
        "reset_fn = planet.create_planet_reset_fn(model=model)\n",
        "observe_fn = planet.create_planet_observe_fn(model=model,\n",
        "                                             model_dir=model_dir,\n",
        "                                             strategy=dist_strategy)\n",
        "predict_fn = planet.create_planet_predict_fn(model=model,\n",
        "                                             strategy=dist_strategy)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'planet' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1262748954.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplanet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRecurrentStateSpaceModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/tmp/experiment/model'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdist_strategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMirroredStrategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mreset_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplanet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_planet_reset_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'planet' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bDItvH82nPW"
      },
      "source": [
        "In addition to `reset_fn`, `observe_fn` and `predict_fn`, we also need to define a `train_fn` as an extra hook to train the model on the latest collected episodes, with this signature: `train_fn(data_path) -> None`. There are utility functions for fast data processing in the `utils/npz.py` that can be used in a training loop but the library is agnostic to how training/checkpointing/restoring is done."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTXpsZUN4RkO"
      },
      "source": [
        "train_steps = 100  # How many training steps per episode\n",
        "batch = 50\n",
        "duration = 50  # How many timesteps in a single training sequence\n",
        "learning_rate = 1e-3\n",
        "\n",
        "train_fn = planet.create_planet_train_fn(model=model,\n",
        "                                         train_steps=train_steps,\n",
        "                                         batch=batch,\n",
        "                                         duration=duration,\n",
        "                                         learning_rate=learning_rate,\n",
        "                                         model_dir=model_dir,\n",
        "                                         strategy=dist_strategy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N93e5Abf9dW7"
      },
      "source": [
        "## Planner"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIkpW1KE9gCh"
      },
      "source": [
        "The *planner* is responsible for decision making. It can use the world model to make predictions about the future and make informed decisions about which actions to take next. We normally need **separate planners** for training and evaluation, since we might need some sort of exploration during training that is not applicable for evaluation. We implemented a few planners in the `planners/planners.py` file including continous and discrete *Cross Entropy Method (CEM)*. Diagram below, shows how CEM iteratively refines itself to choose an optimal action.\n",
        "![CEM iteration](https://i.imgur.com/2iUmnIK.png)\n",
        "\n",
        "A planner also needs an `objective_fn` to compute scores from *world model* predictions. If a world model predicts rewards directly, we can use a `DiscountedReward` objective."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5aj1zWBAY5g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "a5ebf540-940d-4726-8008-10d97e40d501"
      },
      "source": [
        "objective_fn = objectives.DiscountedReward()\n",
        "\n",
        "horizon = 12  # CEM planning horizon\n",
        "iterations = 10  # CEM iterations\n",
        "proposals = 1000  # Number of proposals to evaluate per iteration\n",
        "top_fraction = 0.1  # Fraction of proposals with highest scores for fitting\n",
        "\n",
        "# Base CEM planner to use for evaluation.\n",
        "base_cem = planners.CEM(predict_fn=predict_fn,\n",
        "                        observe_fn=observe_fn,\n",
        "                        reset_fn=reset_fn,\n",
        "                        task=task,\n",
        "                        objective_fn=objective_fn,\n",
        "                        horizon=horizon,\n",
        "                        iterations=iterations,\n",
        "                        proposals=proposals,\n",
        "                        fraction=top_fraction)\n",
        "\n",
        "# Training CEM planner with initial random cold start and random noise.\n",
        "# Pure random actions for the first `n` episodes to bootstrap the world model.\n",
        "random_cold_start_episodes = 5\n",
        "train_cem = planners.RandomColdStart(task=task,\n",
        "                                     random_episodes=random_cold_start_episodes,\n",
        "                                     base_planner=base_cem)\n",
        "# Add some Gaussian noise for active exploration.\n",
        "noise_scale = 0.3\n",
        "train_cem = planners.GaussianRandomNoise(task=task,\n",
        "                                         stdev=noise_scale,\n",
        "                                         base_planner=train_cem)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'objectives' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2323375959.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mobjective_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobjectives\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDiscountedReward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mhorizon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m12\u001b[0m  \u001b[0;31m# CEM planning horizon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0miterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m  \u001b[0;31m# CEM iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mproposals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m  \u001b[0;31m# Number of proposals to evaluate per iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'objectives' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kysNYc7H2v-"
      },
      "source": [
        "## Simulation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDRfdPglH9I2"
      },
      "source": [
        "In order to run an agent on the task, we can use the `simulate` function in the `simulate/simulate.py` file. Below is a diagram of the chain of events during an episode.\n",
        "\n",
        "![Simulation logic](https://i.imgur.com/JjwNHwj.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujXqWewrKZa5"
      },
      "source": [
        "episode_num = 0\n",
        "train_data_dir = '/tmp/experiment/data/train'\n",
        "train_summary_dir = '/tmp/experiment/train'\n",
        "episodes = list()\n",
        "\n",
        "for i in range(random_cold_start_episodes):\n",
        "  episode, predictions, score = simulate.simulate(task=task,\n",
        "                                                  planner=train_cem,\n",
        "                                                  num_episodes=1)\n",
        "  scalar_summaries = {'score': score}\n",
        "  train_eval.visualize(summary_dir=train_summary_dir,\n",
        "                       global_step=i,\n",
        "                       episodes=episode,\n",
        "                       predictions=predictions,\n",
        "                       scalars=scalar_summaries)\n",
        "  episodes.extend(episode)\n",
        "  episode_num += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-vYTuBhAoVN"
      },
      "source": [
        "%tensorboard --logdir=/tmp/experiment/ --port=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiOtedw3MJ67"
      },
      "source": [
        "We normally need to update our *world model* periodically on all the collected episodes so far, therefore we need to interleave simulation with model training. Since the size of collected episodes will grow over time, we should persist them to disk and use optimized/cacheable data iterators for training. Utility functions in `utils/npz.py` can be used here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OegEDO9xN_Dg"
      },
      "source": [
        "npz.save_dictionaries(episodes, train_data_dir)\n",
        "train_fn(train_data_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyBiiTiHPR8N"
      },
      "source": [
        "%tensorboard --logdir=/tmp/experiment/ --port=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvfgWvYqRMdQ"
      },
      "source": [
        "Now we can evaluate our agent by using the `base_planner` that is noise free."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYohNSFMRXUh"
      },
      "source": [
        "eval_summary_dir = '/tmp/experiment/eval'\n",
        "episode, predictions, score = simulate.simulate(task=task,\n",
        "                                                planner=base_cem,\n",
        "                                                num_episodes=1)\n",
        "scalar_summaries = {'score': score}\n",
        "train_eval.visualize(summary_dir=eval_summary_dir,\n",
        "                     global_step=i,\n",
        "                     episodes=episode,\n",
        "                     predictions=predictions,\n",
        "                     scalars=scalar_summaries)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zw0GZ4wuPUxI"
      },
      "source": [
        "%tensorboard --logdir=/tmp/experiment/ --port=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JI9ZKITxCX4B"
      },
      "source": [
        "## Off the Shelf Train-Eval Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXIbN_-zCe6W"
      },
      "source": [
        "A utility function named `train_eval_loop` in `loops/train_eval.py` encapsulates training, evaluating, data collection and tensorboard summary writing all in the same place. If this off the shelf functionality is sufficient we recommend using it instead of implementing them from lower level functions as depicted above.\n",
        "![Architecture diagram](https://i.imgur.com/VjHWDhx.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psKf9kNaUAvl"
      },
      "source": [
        "train_episodes_per_iter = 1  # How many training episodes to collect per train/eval iteration\n",
        "eval_every_n_iters = 10  # A single eval episode every n iterations\n",
        "num_iters = 100  # Total number of train/eval iterations\n",
        "data_dir = '/tmp/experiment/loop/data/'\n",
        "model_dir = '/tmp/experiment/loop/model'\n",
        "\n",
        "train_eval.train_eval_loop(task=task,\n",
        "                           train_planner=train_cem,\n",
        "                           eval_planner=base_cem,\n",
        "                           train_fn=train_fn,\n",
        "                           num_train_episodes_per_iteration=train_episodes_per_iter,\n",
        "                           eval_every_n_iterations=eval_every_n_iters,\n",
        "                           num_iterations=num_iters,\n",
        "                           episodes_dir=data_dir,\n",
        "                           model_dir=model_dir\n",
        "                           )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mPOyAYwPYYE"
      },
      "source": [
        "%tensorboard --logdir=/tmp/experiment/ --port=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1K-3gnqJfRmA"
      },
      "source": [
        "## Gin Configs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bClQv1DBfbmp"
      },
      "source": [
        "Finally, it is important to note that all of the above functionalities described above are gin configurable. However we need to provide a gin config to correctly instantiate the task, model, planner and other parameters to assemble an experiment. There are example configs in the `configs/` folder.\n",
        "\n",
        "The main binary `bin/train_eval.py` defines two bindings for `model_dir=<output_dir>/model` and `episodes_dir=<output_dir>/episodes` that are populated from commandline argument `output_dir`. Any gin configurable component that might need either `model_dir` or `episodes_dir`, should use bindings like `<component>.<property>=%model_dir` instead of hard-coding it in the gin config. Below is an example gin config to instantiate a PlaNet agent.\n",
        "\n",
        "```\n",
        "import google3.learning.brain.research.world_models.api.agents.planet\n",
        "\n",
        "# Parameters for model:\n",
        "RecurrentStateSpaceModel.frame_size = (64, 64, 3)\n",
        "RecurrentStateSpaceModel.reward_stop_gradient = False\n",
        "RecurrentStateSpaceModel.task = %TASK\n",
        "# Use singleton to inject the same instance of model later on.\n",
        "MODEL = @model/singleton()\n",
        "model/singleton.constructor = @RecurrentStateSpaceModel\n",
        "\n",
        "# Parameters for predict, observe and reset\n",
        "STRATEGY = @strategy/singleton()\n",
        "strategy/singleton.constructor = @tf.distribute.MirroredStrategy\n",
        "create_planet_predict_fn.model = %MODEL  # Singleton reference\n",
        "create_planet_predict_fn.strategy = %STRATEGY\n",
        "create_planet_observe_fn.model = %MODEL\n",
        "create_planet_observe_fn.model_dir = %model_dir\n",
        "create_planet_observe_fn.strategy = %STRATEGY\n",
        "create_planet_reset_fn.model = %MODEL\n",
        "\n",
        "# Parameters for train_fn:\n",
        "create_planet_train_fn.train_steps = 100\n",
        "create_planet_train_fn.batch = 50\n",
        "create_planet_train_fn.duration = 50\n",
        "create_planet_train_fn.learning_rate = 1e-3\n",
        "create_planet_train_fn.model_dir = %model_dir  # Is populated from flags.\n",
        "create_planet_train_fn.model = %MODEL\n",
        "create_planet_train_fn.strategy = %STRATEGY\n",
        "```\n"
      ]
    }
  ]
}